{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import model_selection, preprocessing, naive_bayes, metrics\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.models import Sequential\n",
    "from keras import layers, models, optimizers\n",
    "import scipy\n",
    "\n",
    "df_spam = pd.read_csv(\"data/SPAM text message 20170820 - Data.csv\")\n",
    "df_yelp = pd.read_csv(\"data/yelp.csv\")\n",
    "df_corona = pd.read_csv(\"data/Corona_NLP_train.csv\")\n",
    "df_imdb = pd.read_csv(\"data/IMDB Dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Splitting Test/Train spam\n",
    "train_x, test_x, train_y, test_y = model_selection.train_test_split(df_spam['Message'], df_spam['Category'])\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "test_y = encoder.fit_transform(test_y)\n",
    "\n",
    "#Splitting Test/Train yelp\n",
    "train_x2, test_x2, train_y2, test_y2 = model_selection.train_test_split(df_yelp['Message'], df_yelp['Category'])\n",
    "encoder2 = preprocessing.LabelEncoder()\n",
    "train_y2 = encoder2.fit_transform(train_y2)\n",
    "test_y2 = encoder2.fit_transform(test_y2)\n",
    "\n",
    "#Splitting Test/Train corona\n",
    "train_x3, test_x3, train_y3, test_y3 = model_selection.train_test_split(df_corona['OriginalTweet'], df_corona['Sentiment'])\n",
    "encoder3 = preprocessing.LabelEncoder()\n",
    "train_y3 = encoder3.fit_transform(train_y3)\n",
    "test_y3 = encoder3.fit_transform(test_y3)\n",
    "\n",
    "#Splitting Test/Train IMDB\n",
    "train_x4, test_x4, train_y4, test_y4 = model_selection.train_test_split(df_imdb['review'], df_imdb['sentiment'])\n",
    "encoder4 = preprocessing.LabelEncoder()\n",
    "train_y4 = encoder4.fit_transform(train_y4)\n",
    "test_y4 = encoder4.fit_transform(test_y4)\n",
    "\n",
    "#Preprocessing tfidf spam\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "tfidf_vect.fit(df_spam['Message'])\n",
    "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
    "xtest_tfidf =  tfidf_vect.transform(test_x)\n",
    "\n",
    "#Preprocessing tfidf yelp\n",
    "tfidf_vect2 = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "tfidf_vect2.fit(df_yelp['Message'])\n",
    "xtrain_tfidf2 =  tfidf_vect.transform(train_x2)\n",
    "xtest_tfidf2 =  tfidf_vect.transform(test_x2)\n",
    "\n",
    "#Preprocessing tfidf corona\n",
    "tfidf_vect3 = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "tfidf_vect3.fit(df_corona['OriginalTweet'])\n",
    "xtrain_tfidf3 =  tfidf_vect3.transform(train_x3)\n",
    "xtest_tfidf3 =  tfidf_vect3.transform(test_x3)\n",
    "\n",
    "#Preprocessing tfidf IMDB\n",
    "tfidf_vect4 = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "tfidf_vect4.fit(df_imdb['review'])\n",
    "xtrain_tfidf4 =  tfidf_vect4.transform(train_x4)\n",
    "xtest_tfidf4 =  tfidf_vect4.transform(test_x4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9576453697056713\n",
      "0.4895274181665088\n",
      "0.4564625850340136\n",
      "0.85368\n"
     ]
    }
   ],
   "source": [
    "#spam Naive Bayes\n",
    "cls = naive_bayes.MultinomialNB().fit(xtrain_tfidf, train_y)\n",
    "pred = cls.predict(xtest_tfidf)\n",
    "print(metrics.accuracy_score(pred, test_y))\n",
    "\n",
    "#yelp Naive Bayes\n",
    "cls = naive_bayes.MultinomialNB().fit(xtrain_tfidf2, train_y2)\n",
    "pred = cls.predict(xtest_tfidf2)\n",
    "print(metrics.accuracy_score(pred, test_y2))\n",
    "\n",
    "#covid Naive Bayes\n",
    "cls = naive_bayes.MultinomialNB().fit(xtrain_tfidf3, train_y3)\n",
    "pred = cls.predict(xtest_tfidf3)\n",
    "print(metrics.accuracy_score(pred, test_y3))\n",
    "\n",
    "#imdb Naive Bayes\n",
    "cls = naive_bayes.MultinomialNB().fit(xtrain_tfidf4, train_y4)\n",
    "pred = cls.predict(xtest_tfidf4)\n",
    "print(metrics.accuracy_score(pred, test_y4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "for i, line in enumerate(open('data/wiki-news-300d-1M.vec', 'r', encoding='utf8')):\n",
    "    values = line.split()\n",
    "    embeddings_index[values[0]] = np.asarray(values[1:], dtype='float32')\n",
    "\n",
    "# create a tokenizer \n",
    "token = text.Tokenizer()\n",
    "token.fit_on_texts(df_spam['Message'])\n",
    "word_index = token.word_index\n",
    "\n",
    "# convert text to sequence of tokens and pad them to ensure equal length vectors \n",
    "train_seq_x = sequence.pad_sequences(token.texts_to_sequences(train_x), maxlen=70)\n",
    "test_seq_x = sequence.pad_sequences(token.texts_to_sequences(test_x), maxlen=70)\n",
    "\n",
    "# create token-embedding mapping\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131/131 [==============================] - 2s 13ms/step - loss: 0.2375\n",
      "0.8521177315147165\n"
     ]
    }
   ],
   "source": [
    "def create_cnn():\n",
    "    # Add an Input Layer\n",
    "    input_layer = layers.Input((70, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "\n",
    "    # Add the convolutional Layer\n",
    "    conv_layer = layers.Convolution1D(100, 3, activation=\"relu\")(embedding_layer)\n",
    "\n",
    "    # Add the pooling Layer\n",
    "    pooling_layer = layers.GlobalMaxPool1D()(conv_layer)\n",
    "\n",
    "    # Add the output Layers\n",
    "    output_layer1 = layers.Dense(50, activation=\"relu\")(pooling_layer)\n",
    "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
    "    \n",
    "    return model\n",
    "\n",
    "cls = create_cnn()\n",
    "cls.fit(train_seq_x, train_y)\n",
    "pred = cls.predict(test_seq_x)\n",
    "pred = pred.argmax(axis=-1)\n",
    "print(metrics.accuracy_score(pred, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
